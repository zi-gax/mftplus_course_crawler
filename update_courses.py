import aiohttp
import asyncio
import pandas as pd
import json
import os
from datetime import datetime
from zoneinfo import ZoneInfo
from urllib.parse import quote

# ---------------- Configuration ----------------
TEHRAN_TZ = ZoneInfo("Asia/Tehran")
API_URL = "https://mftplus.com/ajax/default/calendar?need=search"
PAGE_SIZE = 9
MAX_CONCURRENCY = 5
MAX_EMPTY_PAGES = 2

CSV_FILE = "mftplus_courses_async.csv"
JSON_FILE = "mftplus_courses_async.json"
LOG_FILE = "COURSE_LOG.md"

HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "X-Requested-With": "XMLHttpRequest",
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
    "Referer": "https://mftplus.com/calendar"
}

# ---------------- Stage 1: Fetch ----------------
async def fetch_page(session, skip):
    payload = {"term": "", "sort": "", "skip": skip, "pSkip": 0, "type": "all"}
    try:
        async with session.post(API_URL, data=payload) as resp:
            return json.loads(await resp.text())
    except Exception as e:
        print(f"âš ï¸ skip={skip} error: {e}")
        return []

async def fetch_all_courses():
    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENCY)
    courses, skip, empty = [], 0, 0

    async with aiohttp.ClientSession(headers=HEADERS, connector=connector) as session:
        while True:
            data = await fetch_page(session, skip)
            if not data:
                empty += 1
                if empty >= MAX_EMPTY_PAGES:
                    break
            else:
                empty = 0
                courses.extend(data)
                print(f"âœ… skip={skip} â†’ {len(data)} courses")

            skip += PAGE_SIZE
            await asyncio.sleep(0.2)

    return courses

# ---------------- Stage 2: Normalize ----------------
def make_course_link(course):
    return (
        f"https://mftplus.com/lesson/"
        f"{course.get('lessonId','')}/"
        f"{course.get('lessonUrl','')}?refp={quote(course.get('center',''))}"
    )

def normalize(course, is_active, changed_at):
    now = datetime.now(TEHRAN_TZ).strftime("%Y-%m-%d %H:%M:%S")
    return {
        "id": course["id"]["$oid"],
        "title": course.get("title", ""),
        "department": course.get("dep", ""),
        "center": course.get("center", ""),
        "teacher": course.get("author", ""),
        "start_date": course.get("start", ""),
        "end_date": course.get("end", ""),
        "capacity": course.get("capacity", ""),
        "duration_hours": course.get("time", ""),
        "days": " | ".join(course.get("days", [])),
        "min_price": course.get("minCost", ""),
        "max_price": course.get("maxCost", ""),
        "course_url": make_course_link(course),
        "cover": course.get("cover", ""),
        "is_active": is_active,
        "changed_at": changed_at,
        "updated_at": now
    }

# ---------------- Stage 3: Load ----------------
def load_existing():
    if os.path.exists(CSV_FILE):
        return pd.read_csv(CSV_FILE)
    return pd.DataFrame()

# ---------------- Stage 4: Save ----------------
def save_all(df, new_courses, expired_courses, revived_courses):
    df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
    df.to_json(JSON_FILE, force_ascii=False, indent=2)
    print("ğŸ’¾ CSV & JSON updated")

    now = datetime.now(TEHRAN_TZ).strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"\n<details>\n<summary>ğŸ“Š Sync {now} ğŸ“ˆ({len(new_courses)})|ğŸ“‰({len(expired_courses)})|â™»ï¸({len(revived_courses)})</summary>\n\n")

        for title, items, emoji in [
            ("New courses", new_courses, "ğŸ“ˆ"),
            ("Expired courses", expired_courses, "ğŸ“‰"),
            ("Revived courses", revived_courses, "â™»ï¸")
        ]:
            if items:
                f.write(f"\n<details>\n<summary>{emoji} {title} ({len(items)})</summary>\n\n")
                for c in items:
                    f.write(f"- [{c['title']}]({c['course_url']}) | {c['center']}\n")
                f.write("</details>\n")

        f.write("</details>\n")

# ---------------- Main ----------------
async def main():
    existing_df = load_existing()
    now = datetime.now(TEHRAN_TZ).strftime("%Y-%m-%d %H:%M:%S")

    existing_map = {
        str(row["id"]): row.to_dict()
        for _, row in existing_df.iterrows()
    }

    old_active_ids = {
        str(row["id"])
        for _, row in existing_df.iterrows()
        if row.get("is_active") == 1
    }

    old_inactive_ids = {
        str(row["id"])
        for _, row in existing_df.iterrows()
        if row.get("is_active") == 0
    }

    raw_courses = await fetch_all_courses()
    print(f"ğŸ¯ Total fetched: {len(raw_courses)}")

    api_ids = set()
    api_courses = []

    for c in raw_courses:
        cid = c["id"]["$oid"]
        api_ids.add(cid)

        prev = existing_map.get(cid)
        status_changed = not prev or prev["is_active"] == 0

        api_courses.append(
            normalize(
                c,
                is_active=1,
                changed_at=now if status_changed else prev.get("changed_at")
            )
        )

    new_courses = [c for c in api_courses if c["id"] not in existing_map]
    revived_courses = [c for c in api_courses if c["id"] in old_inactive_ids]

    expired_courses = []
    for cid in old_active_ids - api_ids:
        row = existing_map[cid]
        row["is_active"] = 0
        row["changed_at"] = now
        row["updated_at"] = now
        expired_courses.append(row)

    # ---------------- FINAL MERGE (NO DUPLICATES) ----------------
    final_map = {}

    for cid, row in existing_map.items():
        final_map[cid] = row

    for c in api_courses:
        final_map[c["id"]] = c

    for c in expired_courses:
        final_map[c["id"]] = c

    final_df = pd.DataFrame(final_map.values())

    save_all(final_df, new_courses, expired_courses, revived_courses)

    print(f"âœ¨ New: {len(new_courses)}")
    print(f"â¸ï¸ Expired: {len(expired_courses)}")
    print(f"â™»ï¸ Revived: {len(revived_courses)}")

if __name__ == "__main__":
    asyncio.run(main())
